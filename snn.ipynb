{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schritt 1: Alle nötigen Imports importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from norse.torch import LIFCell, LIFParameters, LIFState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schritt 2: Implementierung des SNN und des LIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP_SNN(nn.Module):\n",
    "    # Definition des SNN und Integration des LIF-Modells\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, tau_plus=20.0, tau_minus=20.0):\n",
    "        super(STDP_SNN, self).__init__()\n",
    "\n",
    "        # LIF-Parameter (normalerweise sollten diese konstant bleiben)\n",
    "        lif_params = LIFParameters()\n",
    "        self.lif1 = LIFCell(p=lif_params)\n",
    "        self.synapse_weights = torch.randn(input_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Normalisierung der Einagbedaten\n",
    "        nn.init.xavier_normal_(self.synapse_weights)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "\n",
    "        # Aktivierungsfunktion für die verborgene Schicht\n",
    "        self.hidden_activation = nn.ReLU()\n",
    "\n",
    "    # Lernmethode implementieren\n",
    "    def apply_stdp(self, prespike_time, postsynaptic_time):\n",
    "        time_difference = postsynaptic_time - prespike_time\n",
    "        weight_increase = torch.exp(-torch.abs(time_difference) / self.tau_plus)\n",
    "        weight_decrease = torch.exp(-torch.abs(time_difference) / self.tau_minus)\n",
    "        weight_delta = self.learning_rate * (weight_increase - weight_decrease)\n",
    "        weight_delta_expanded = weight_delta.view(self.synapse_weights.shape)\n",
    "        self.synapse_weights += weight_delta_expanded\n",
    "\n",
    "    # Verlustfunktion definieren\n",
    "    def apply_loss_function(self, y_true, y_pred):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        return nn.BCELoss()(y_pred, y_true)\n",
    "\n",
    "    # Optimierungsfunktion definieren\n",
    "    def apply_opt_function(self, loss):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # Normalisierung der Eingabedaten\n",
    "    def normalize_input(self, data):\n",
    "        return (data - data.mean()) / data.std()\n",
    "\n",
    "    # Starten des SNN\n",
    "    def forward(self, x):\n",
    "        x = self.normalize_input(x)\n",
    "        spikes = self.lif1(x)\n",
    "        hidden_output = self.hidden_activation(spikes)\n",
    "        output = self.fc2(hidden_output)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
